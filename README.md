# Thesis-Artifacts

This repository includes the artifacts and scripts for evaluating the Flexflow and FasterTransformer.

## Flexflow GPT3-MoE
Clone and Build
```
git clone --recursive https://github.com/flexflow/FlexFlow.git -b modelscope
mkdir -p FlexFlow/build
cd FlexFlow/build
export FF_BUILD_ALL_EXAMPLES=ON
export FF_BUILD_ALL_INFERENCE_EXAMPLES=ON
source activate
../config/config.linux 
make -j
``` 

Generate input workload files:
```
cd ../examples/cpp/inference/
g++ data_generator.cpp data_generator.cc -I../../../include
./a.out
```

Run test:
./examples/cpp/inference/mixture_of_experts/inference_moe -ll:cpu 4 -ll:gpu 4 -ll:util 16 -ll:fsize 14192 -ll:zsize 12192 --only-data-parallel --dataset <PATH TO request_tokens.txt> --arrival_info_path <PATH to arrival_times.txt> --tokens_to_generate <PATH to tokens_to_generate.txt> > flex_4_gpu_log.txt
```

To parser the results, run:
```
python3 flex_parser.py --input flex_4_gpu_log.txt --gpu 4 --arrival arrival_times.txt
```

## FasterTransformer GPT3-MoE Eval 

Run following command to build FasterTransformer
```
sudo nvidia-docker run -ti --gpus all --shm-size 5g  --rm nvcr.io/nvidia/pytorch:22.07-py3 bash
git clone https://github.com/NVIDIA/FasterTransformer.git
mkdir -p FasterTransformer/build
cd FasterTransformer/build
git submodule init && git submodule update
cmake -DSM=75 -DCMAKE_BUILD_TYPE=Release -DBUILD_PYT=ON -DBUILD_MULTI_GPU=ON ..
make -j12
```

install dependencies for GPT3-MoE examples. 
```
wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json -P ../models
wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt -P ../models
pip install git+https://github.com/microsoft/DeepSpeed.git
git clone https://github.com/byshiue/Megatron-DeepSpeed/ -b moe_ft
pip install Megatron-DeepSpeed/
pip install jieba
pip install -r ../examples/pytorch/gpt/requirement.txt
```

Prepare Models
```
git lfs clone https://www.modelscope.cn/PAI/nlp_gpt3_text-generation_0.35B_MoE-64.git
mv nlp_gpt3_text-generation_0.35B_MoE-64 ../models


PYTHONPATH=$PWD/../ python ../examples/pytorch/gpt/utils/megatron_gpt_moe_ckpt_convert.py \
                    --input-dir ../models/nlp_gpt3_text-generation_0.35B_MoE-64/model \
                    --saved-dir ../models/nlp_gpt3_text-generation_0.35B_MoE-64/model/c-models \
                    --infer-gpu-num 4 \
                    --vocab-path ../models/gpt2-vocab.json \
                    --merges-path ../models/gpt2-merges.txt
```
Make sure to update the `config.ini` and `args.txt` under `FasterTransformer/models/nlp_gpt3_text-generation_0.35B_MoE-64/model/c-models/4-gpu/`

Copy modified muti_gpu_example.py from outside of the container.
```
sudo docker cp ./multi_gpu_gpt_example.py <CONTAINER_ID>:/workspace/FasterTransformer/examples/pytorch/gpt/modified_example.py
```

Copy input workload files generated by Flexflow data_generator.
```
sudo docker cp ./arrival_times.txt  <CONTAINER_ID>:/workspace/FasterTransformer/build/
sudo docker cp ./tokens_to_generate.txt <CONTAINER_ID>:/workspace/FasterTransformer/build/
sudo docker cp ./request_tokens.txt <CONTAINER_ID>:/workspace/FasterTransformer/build/
```

Run 4-gpu Test:
```
mpirun --allow-run-as-root -n 4 \
python3  ../examples/pytorch/gpt/modified_example.py \
        --tensor_para_size=1\
        --pipeline_para_size=1 \
        --ckpt_path=../models/c-models/4-gpu/ \
        --data_type=fp16 \
        --vocab_file=../models/tokenizer.json \
        --vocab_size=51200 \
        --start_id=7 \
        --end_id=7 \
        --max_batch_size=8 \
        --output_length_file=tokens_to_generate.txt \
        --arrival_info_file=arrival_times.txt \
        --request_input_file=request_tokens.txt \
        --time \
        --use_jieba_tokenizer > fast_4_gpu_log.txt
```

To parser the results, run:
```
python3 fast_parser.py --input fast_4_gpu_log.txt --gpu 4
```

