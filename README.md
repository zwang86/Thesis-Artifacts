# Thesis-Artifacts

This repository includes the artifacts and scripts for evaluating the Flexflow and FasterTransformer.


## FasterTransformer GPT3-MoE Eval 

Run following command to build FasterTransformer
```
sudo nvidia-docker run -ti --gpus all --shm-size 5g  --rm nvcr.io/nvidia/pytorch:22.07-py3 bash
git clone https://github.com/NVIDIA/FasterTransformer.git
mkdir -p FasterTransformer/build
cd FasterTransformer/build
git submodule init && git submodule update
cmake -DSM=75 -DCMAKE_BUILD_TYPE=Release -DBUILD_PYT=ON -DBUILD_MULTI_GPU=ON ..
make -j12
```

install dependencies for GPT3-MoE examples. 
```
wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json -P ../models
wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt -P ../models
pip install git+https://github.com/microsoft/DeepSpeed.git
git clone https://github.com/byshiue/Megatron-DeepSpeed/ -b moe_ft
pip install Megatron-DeepSpeed/
pip install jieba
pip install -r ../examples/pytorch/gpt/requirement.txt
```

Prepare Models
```
git lfs clone https://www.modelscope.cn/PAI/nlp_gpt3_text-generation_0.35B_MoE-64.git
mv nlp_gpt3_text-generation_0.35B_MoE-64 ../models


PYTHONPATH=$PWD/../ python ../examples/pytorch/gpt/utils/megatron_gpt_moe_ckpt_convert.py \
                    --input-dir ../models/nlp_gpt3_text-generation_0.35B_MoE-64/model \
                    --saved-dir ../models/nlp_gpt3_text-generation_0.35B_MoE-64/model/c-models \
                    --infer-gpu-num 4 \
                    --vocab-path ../models/gpt2-vocab.json \
                    --merges-path ../models/gpt2-merges.txt
```
Make sure to update the `config.ini` and `args.txt` under `FasterTransformer/models/nlp_gpt3_text-generation_0.35B_MoE-64/model/c-models/4-gpu/'

Copy modified muti_gpu_example.py from outside of the container.
```
sudo docker cp ./multi_gpu_gpt_example.py 0c11045bd1ab:/workspace/FasterTransformer/examples/pytorch/gpt/modified_example.py
```

Copy input workload files generated by Flexflow data_generator.
```
sudo docker cp ./arrival_times.txt  0c11045bd1ab:/workspace/FasterTransformer/build/
sudo docker cp ./tokens_to_generate.txt 0c11045bd1ab:/workspace/FasterTransformer/build/
sudo docker cp ./request_tokens.txt 0c11045bd1ab:/workspace/FasterTransformer/build/
```

Run 4-gpu Test:
```
mpirun --allow-run-as-root -n 4 \
python3  ../examples/pytorch/gpt/modified_example.py \
        --tensor_para_size=1\
        --pipeline_para_size=1 \
        --ckpt_path=../models/c-models/4-gpu/ \
        --data_type=fp16 \
        --vocab_file=../models/tokenizer.json \
        --vocab_size=51200 \
        --start_id=7 \
        --end_id=7 \
        --max_batch_size=8 \
        --output_length_file=tokens_to_generate.txt \
        --arrival_info_file=arrival_times.txt \
        --request_input_file=request_tokens.txt \
        --time \
        --use_jieba_tokenizer > gpu_4_align.txt
```


